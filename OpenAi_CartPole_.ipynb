{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #                                               OPEN AI GYM CARTPOLE V1\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the following program we shall train a neural network to learn to play the game 'Cart-Pole'.This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "#Lets start by importing all our dependecies\n",
    "import gym          #OpenAi Game environment\n",
    "import random\n",
    "import numpy as np\n",
    "import tflearn      #Machine learning package used to build out neural network\n",
    "from tflearn.layers.core import input_data,dropout,fully_connected #Layers needed for the network\n",
    "from tflearn.layers.estimator import regression     \n",
    "from statistics import mean , median    #For statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining network parameters\n",
    "\n",
    "LR=1e-3             #LEARNING RATE\n",
    "goal_setps= 500     #update to 200 later\n",
    "score_req = 50      #can update later\n",
    "initial_games =5000 #No of games to play to generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <type 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01259498, -0.04456971, -0.04837084,  0.01008032])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing the game environment and type\n",
    "\n",
    "env=gym.make('CartPole-v1')  #Importing game env\n",
    "env.reset()                  #Starts/Re-starts the game Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a feel for Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined out network parameters and initialized our environment,lets see what it looks like.\n",
    "The following function makes random moves as it renders the game for 10 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_games():\n",
    "\n",
    "    for episode in range(10):\n",
    "        env.reset()\n",
    "        score=0\n",
    "\n",
    "        for t in range(goal_setps):\n",
    "\n",
    "            env.render()\n",
    "\n",
    "            action=env.action_space.sample()\n",
    "\n",
    "            observation , reward , done , info =env.step(action)\n",
    "            #print(observation , reward , done)\n",
    "            score+=reward\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Executing this cell will run the above function rendering the game \n",
    "\n",
    "random_games()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train any ML model we need training data.\n",
    "In this case we shall generate our own training data but recording observation for the games we shall play.The following function ' initial_population ' will help us populate the training_data list.We add only the observations of the gamw where we have achieved a score of more than the requried score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a list of populated training data\n",
    "def initial_population():\n",
    "    \n",
    "    \n",
    "    \n",
    "    training_data=[]\n",
    "    scores=[]\n",
    "    accepted_scores=[] #Going to accept scores above 50\n",
    "    \n",
    "    for _ in range(initial_games):\n",
    "        #Each new game starts with a fresh score-board and game_memory\n",
    "        score=0\n",
    "        game_memory=[]\n",
    "        prev_observation=[]\n",
    "\n",
    "        for _ in range(goal_setps):\n",
    "            \n",
    "            #In this loop we iterate over fames in each game(episode)\n",
    "            \n",
    "            #We pick a random action between 0 and 1 which corresponds to left and right\n",
    "            action=random.randrange(0,2)\n",
    "            '''\n",
    "            This action is used to take a \"step\" in the env,which returns\n",
    "            observation(dtype:list),\n",
    "            reward(dtype:int) > it is 1 if the cart is able to survive the current frame or else its 0\n",
    "            done(dtype:bool)\n",
    "            info contains information about the step,if needed can be logged but we arent going to be needing it.\n",
    "             '''\n",
    "            observation , reward , done , info =env.step(action)\n",
    "            \n",
    "            \n",
    "            #We shall now store the observation into the game memory along with the action which was taken to obtain that state\n",
    "            if len(prev_observation) > 0:\n",
    "                game_memory.append([prev_observation,action])\n",
    "            #Updating observation and rewarding the scoreaccordingly\n",
    "            prev_observation = observation\n",
    "            score += reward\n",
    "            \n",
    "            #done = True ,when episode is complete and hence we break from this loop\n",
    "            if done:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        #Now we shall only store the data of games where we have a score of more than the requried score.\n",
    "        \n",
    "        if score >= score_req:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory:\n",
    "                '''\n",
    "                Converting to one hot output.\n",
    "                Where the category to which the data belongs to will be True(i.e- 1) and other categories shall be false(i.e 0)\n",
    "                We are prefereably using this as we may encounter \n",
    "                categorial data of more than 2 categories in future cases.\n",
    "                '''\n",
    "                if data[1]==1:\n",
    "                    output = [0,1]\n",
    "                elif data[1]==0:\n",
    "                    output=[1,0]\n",
    "                #Appending the training data\n",
    "                training_data.append([data[0],output])\n",
    "        #Restting the env for next game\n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "    \n",
    "    #Statistical data\n",
    "    print('Avg accepted score',mean(accepted_scores))\n",
    "    print('Median accepted score',median(accepted_scores))\n",
    "    \n",
    "\n",
    "\n",
    "    return training_data\n",
    "    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now create our neural network model using \n",
    "all our imported layers from tflearn.The following function shall satisfy requriments and returning the requried model.\n",
    " - Our networks shall have 5 hidden layers of sizes 128 , 256 , 512 , 256 , 128  nodes respectively.\n",
    " - We shall we using 'Relu' as out activation function and a dropout of 30% on the nodes.\n",
    " - The output layer shall consist of 2 nodes activated by softmax function.\n",
    " - We shall then perfrom regression on the followinf model to optimize for categorical_crossentropy loss using 'Adam' optimizer.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(input_size):\n",
    "    network = input_data(shape = [None, input_size,1] ,name='input')\n",
    "\n",
    "    network=fully_connected(network,128,activation='relu')\n",
    "    network=dropout(network,0.7)\n",
    "\n",
    "    network=fully_connected(network,256,activation='relu')\n",
    "    network=dropout(network,0.7)\n",
    "\n",
    "    network=fully_connected(network,512,activation='relu')\n",
    "    network=dropout(network,0.7)\n",
    "\n",
    "    network=fully_connected(network,256,activation='relu')\n",
    "    network=dropout(network,0.7)\n",
    "\n",
    "    network=fully_connected(network,128,activation='relu')\n",
    "    network=dropout(network,0.7)\n",
    "\n",
    "\n",
    "    network = fully_connected(network , 2 ,activation='softmax')\n",
    "    network = regression(network,optimizer='adam',learning_rate=LR,loss='categorical_crossentropy',name='targets')\n",
    "\n",
    "    model=tflearn.DNN(network)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time To Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data and our neural network model ready we can finally train it!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data,model=False):\n",
    "    #model=False is default as we do not expect to have a model on the 1st run,however a pre-trainined model can also be used.\n",
    "    \n",
    "    #0th index of training_data contains direction of motion L/R\n",
    "    X=np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "    \n",
    "    #1st element of training_data contains the output\n",
    "    y=np.array([i[1] for i in training_data])\n",
    "\n",
    "    #If we do not have a previously defined model we shall define it here.\n",
    "    if not model:\n",
    "        model= neural_network_model(input_size=len(X[0]))\n",
    "\n",
    "    #The NN is fed the training the data in X and output labels in y to fins a function to map the 2\n",
    "    model.fit({'input':X},{'targets':y},n_epoch=3,snapshot_step=500,show_metric=True,run_id='openaiCartPoleV1')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.67398\u001b[0m\u001b[0m | time: 1.689s\n",
      "| Adam | epoch: 003 | loss: 0.67398 - acc: 0.5936 -- iter: 11264/11269\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.67081\u001b[0m\u001b[0m | time: 1.700s\n",
      "| Adam | epoch: 003 | loss: 0.67081 - acc: 0.6045 -- iter: 11269/11269\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Call the above function to train the model\n",
    "\n",
    "training_data = initial_population()\n",
    "model = train_model(training_data=training_data)\n",
    "\n",
    "#In case you wanna save the model for future uses.\n",
    "#model.save(\"CartPoleV1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally Test Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built and trained our model lets see how it performs.We will be logging the choices made(L/R) and the scores for validating out model.\n",
    "\n",
    "According to the documentation on achieving a score of 195 over 100 games,this problem is considered solved.\n",
    "\n",
    "Let's see if we can get there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "choices=[]\n",
    "\n",
    "for each_game in range(100):\n",
    "    score=0\n",
    "    game_memory=[]\n",
    "    prev_obs=[]\n",
    "    env.reset()\n",
    "    for _ in range(goal_setps):\n",
    "\n",
    "        #env.render()\n",
    "        if len(prev_obs) == 0:\n",
    "            action=random.randrange(0,2)\n",
    "        else:\n",
    "            model_prediction=model.predict(prev_obs.reshape(-1,len(prev_obs),1))\n",
    "            action = np.argmax(model_prediction[0])\n",
    "\n",
    "        choices.append(action)\n",
    "\n",
    "        new_obs , reward,done,info=env.step(action)\n",
    "        prev_obs=new_obs\n",
    "        game_memory.append([new_obs,action])\n",
    "\n",
    "        score+=reward\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Score Sheet \\n', [246.0, 212.0, 136.0, 156.0, 252.0, 162.0, 196.0, 153.0, 225.0, 218.0, 258.0, 133.0, 199.0, 217.0, 171.0, 136.0, 184.0, 166.0, 238.0, 236.0, 325.0, 224.0, 186.0, 130.0, 132.0, 429.0, 272.0, 234.0, 206.0, 500.0, 120.0, 248.0, 160.0, 196.0, 151.0, 308.0, 196.0, 220.0, 196.0, 220.0, 141.0, 165.0, 500.0, 217.0, 119.0, 150.0, 232.0, 266.0, 206.0, 160.0, 190.0, 220.0, 436.0, 190.0, 416.0, 148.0, 319.0, 196.0, 448.0, 306.0, 170.0, 266.0, 130.0, 218.0, 130.0, 223.0, 159.0, 274.0, 176.0, 216.0, 268.0, 229.0, 368.0, 354.0, 243.0, 284.0, 226.0, 284.0, 273.0, 184.0, 209.0, 221.0, 500.0, 208.0, 299.0, 286.0, 296.0, 476.0, 239.0, 217.0, 374.0, 186.0, 324.0, 188.0, 209.0, 234.0, 172.0, 232.0, 211.0, 500.0])\n",
      "('Avgerage Score:', 238.28)\n",
      "Choice 1: 0.49294947121 , Choice 2 : 0.50705052879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Score Sheet \\n\",scores)\n",
    "print('Avgerage Score:',float(sum(scores))/len(scores))\n",
    "print(\"Choice 1: {} , Choice 2 : {}\".format(float(choices.count(1))/len(choices),float(choices.count(0))/len(choices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
